{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e6fc6270",
   "metadata": {},
   "source": [
    "게임 플랫폼 스팀에 등록된 한국어 리뷰에 대해서 감성 분석을 진행해보겠습니다. \n",
    "이 데이터는 긍정인 리뷰에는 레이블 1이, 부정인 리뷰에는 레이블 0이 부여되어져 있습니다. 앞서 진행했던 감성 분류 문제들과 마찬가지로 이진 분류 문제를 풉니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58364a9d",
   "metadata": {},
   "source": [
    "# 1. BiLSTM을 텍스트 분류에 사용하기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6345b130",
   "metadata": {},
   "source": [
    "양방향 LSTM은 두 개의 독립적인 LSTM 아키텍처를 함께 사용하는 구조입니다. 위 그림에서 주황색 LSTM 셀은 순차적으로 입력을 받습니다. 자연어 처리라고 한다면, 마치 사람처럼 문장을 왼쪽 단어부터 순차적으로 읽는 셈입니다. \n",
    "양방향 LSTM은 뒤의 문맥까지 고려하기 위해서 문장을 오른쪽에서 반대로 읽는 역방향의 LSTM 셀(위 그림에서 초록색)을 함께 사용합니다. \n",
    "이 두 가지 정보를 고려하여 출력층에서 예측 시에 두 가지 정보를 모두 사용합니다.\n",
    "\n",
    "위 그림은 다 대 다(many-to-many) 문제를 푸는 경우의 양방향 LSTM을 보여주고 있습니다. 그런데 양방향 LSTM을 다 대 일(many-to-one) 문제인 텍스트 분류 문제에 사용한다고 하면, 한 가지 의문이 생깁니다."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d334ac1",
   "metadata": {},
   "source": [
    "일반적으로 순방향 LSTM은 마지막 시점의 은닉 상태를 출력층으로 보내서 텍스트 분류를 수행합니다. \n",
    "그렇다면 역방향 LSTM도 순방향과 같은 시점의 은닉 상태를 사용하면 될까요? \n",
    "위 그림과 같이 텍스트 분류를 진행하는 경우, 역방향 LSTM은 x4만 본 상태입니다. 이 경우 역방향 LSTM이 텍스트 분류를 위한 유용한 정보를 갖고 있다고 기대하기 어렵습니다."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba25e2c0",
   "metadata": {},
   "source": [
    "그래서 케라스에서는 양방향 LSTM을 사용하면서 return_sequences=False를 택할 경우에는 위의 그림과 같이 동작하고 있습니다. \n",
    "순방향 LSTM의 경우에는 마지막 시점의 은닉 상태를 반환하고, 역방향 LSTM의 경우에는 첫번째 시점의 은닉 상태를 반환합니다. 위 구조를 통해서 양방향 LSTM으로 텍스트 분류를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6bf5c",
   "metadata": {},
   "source": [
    "# 2. Mecab 설치"
   ]
  },
  {
   "cell_type": "raw",
   "id": "754a742d",
   "metadata": {},
   "source": [
    "형태소 분석기 Mecab을 사용합니다.\n",
    "Mecab 설치 : https://cleancode-ws.tistory.com/97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64938b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0eb1503",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromedriver_autoinstaller'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver \u001b[38;5;66;03m# 웹 브라우저 자동화\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keys \u001b[38;5;66;03m# 웹 드라이버 동작 명령\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromedriver_autoinstaller\u001b[39;00m \u001b[38;5;66;03m# 크롬 브라우저\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_notebook \u001b[38;5;66;03m# 시간 측정\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;66;03m# 시간 지연\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromedriver_autoinstaller'"
     ]
    }
   ],
   "source": [
    "import sys # 시스템\n",
    "import os  # 시스템\n",
    "\n",
    "# 데이터 처리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup # html 전처리\n",
    "from selenium import webdriver # 웹 브라우저 자동화\n",
    "from selenium.webdriver.common.keys import Keys # 웹 드라이버 동작 명령\n",
    "import chromedriver_autoinstaller # 크롬 브라우저\n",
    "\n",
    "from tqdm import tqdm_notebook # 시간 측정\n",
    "import time # 시간 지연\n",
    "\n",
    "import warnings # 경고문 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187429ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '하', '세요', '오늘', '날씨', '가', '좋', '네요']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab('C:\\mecab\\mecab-ko-dic')\n",
    "mecab.morphs('안녕하세요 오늘 날씨가 좋네요')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901199c3",
   "metadata": {},
   "source": [
    "# 3. 스팀 리뷰 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "raw",
   "id": "737103ab",
   "metadata": {},
   "source": [
    "다운로드 링크 : https://github.com/bab2min/corpus/tree/master/sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7350c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 워닝 무시\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916664ea",
   "metadata": {},
   "source": [
    "## 1) 데이터 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabad5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/steam.txt\", filename=\"steam.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b658606",
   "metadata": {},
   "source": [
    "해당 데이터에는 열제목이 별도로 없습니다. 그래서 임의로 두 개의 열제목인 'label'과 'reviews'를 추가해주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.read_table('steam.txt', names=['label', 'reviews'])\n",
    "\n",
    "print('전체 리뷰 개수 :',len(total_data)) # 전체 리뷰 개수 출력\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "028f06c8",
   "metadata": {},
   "source": [
    "각 열에 대해서 중복을 제외한 샘플의 수를 카운트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data['reviews'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce0e7b30",
   "metadata": {},
   "source": [
    "reviews열에서 중복을 제외한 경우 99,892개입니다. \n",
    "현재 10만개의 리뷰가 존재하므로 이는 현재 갖고 있는 데이터에 중복인 샘플들이 있다는 의미입니다. \n",
    "중복인 샘플들을 제거해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data.drop_duplicates(subset=['reviews'], inplace=True) # reviews 열에서 중복인 내용이 있다면 중복 제거\n",
    "print('총 샘플의 수 :',len(total_data))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71dc5522",
   "metadata": {},
   "source": [
    "NULL 값 유무를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad41c4",
   "metadata": {},
   "source": [
    "## 2) 훈련 데이터와 테스트 데이터 분리하기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0552b579",
   "metadata": {},
   "source": [
    "훈련 데이터와 테스트 데이터를 3:1 비율로 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\n",
    "print('훈련용 리뷰의 개수 :', len(train_data))\n",
    "print('테스트용 리뷰의 개수 :', len(test_data))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76bd27e5",
   "metadata": {},
   "source": [
    "훈련용 리뷰의 경우 약 7만 5,000개. 테스트용 리뷰의 경우 약 2만 5,000개가 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3ba10",
   "metadata": {},
   "source": [
    "## 3) 레이블의 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['label'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.groupby('label').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7aca573",
   "metadata": {},
   "source": [
    "두 레이블 모두 약 3만 7천개로 50:50 비율을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad85c5c",
   "metadata": {},
   "source": [
    "## 4) 데이터 정제하기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "884a5e57",
   "metadata": {},
   "source": [
    "정규 표현식을 사용하여 한글을 제외하고 모두 제거해줍니다. \n",
    "또한 혹시 이 과정에서 빈 샘플이 생기지는 않는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train_data['reviews'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2fca476",
   "metadata": {},
   "source": [
    "테스트 데이터에 대해서도 같은 과정을 거칩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d27d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop_duplicates(subset = ['reviews'], inplace=True) # 중복 제거\n",
    "test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b07b447",
   "metadata": {},
   "source": [
    "불용어를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9900bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '게임', '겜', '되', '음', '면']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd151d",
   "metadata": {},
   "source": [
    "## 5) 토큰화"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e14fa0a2",
   "metadata": {},
   "source": [
    "형태소 분석기 Mecab을 사용하여 토큰화 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6da66d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mecab\n\u001b[0;32m      2\u001b[0m mecab \u001b[38;5;241m=\u001b[39m Mecab(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmecab\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmecab-ko-dic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(mecab\u001b[38;5;241m.\u001b[39mmorphs)\n\u001b[0;32m      5\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords])\n\u001b[0;32m      6\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(mecab\u001b[38;5;241m.\u001b[39mmorphs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab('C:\\mecab\\mecab-ko-dic')\n",
    "\n",
    "train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\n",
    "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)\n",
    "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f35336",
   "metadata": {},
   "source": [
    "## 6) 단어와 길이 분포 확인하기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82aa489f",
   "metadata": {},
   "source": [
    "긍정 리뷰에는 주로 어떤 단어들이 많이 등장하고, 부정 리뷰에는 주로 어떤 단어들이 등장하는지 두 가지 경우에 대해서 각 단어의 빈도수를 계산해보겠습니다. 각 레이블에 따라서 별도로 단어들의 리스트를 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
    "positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87b2c0cc",
   "metadata": {},
   "source": [
    "Counter()를 사용하여 각 단어에 대한 빈도수를 카운트합니다. \n",
    "우선 부정 리뷰에 대해서 빈도수가 높은 상위 20개 단어들을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_word_count = Counter(negative_words)\n",
    "print(negative_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41d47d96",
   "metadata": {},
   "source": [
    "긍정 리뷰에 대해서도 동일하게 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_word_count = Counter(positive_words)\n",
    "print(positive_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a24ddf6",
   "metadata": {},
   "source": [
    "두 가지 경우에 대해서 각각 길이 분포를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\n",
    "ax1.hist(text_len, color='red')\n",
    "ax1.set_title('Positive Reviews')\n",
    "ax1.set_xlabel('length of samples')\n",
    "ax1.set_ylabel('number of samples')\n",
    "print('긍정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "\n",
    "text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\n",
    "ax2.hist(text_len, color='blue')\n",
    "ax2.set_title('Negative Reviews')\n",
    "fig.suptitle('Words in texts')\n",
    "ax2.set_xlabel('length of samples')\n",
    "ax2.set_ylabel('number of samples')\n",
    "print('부정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "696caf3b",
   "metadata": {},
   "source": [
    "유의미한 차이가 있는 것 같지는 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values\n",
    "X_test= test_data['tokenized'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607b21b",
   "metadata": {},
   "source": [
    "## 7) 정수 인코딩"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e371adc8",
   "metadata": {},
   "source": [
    "기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 합니다. \n",
    "우선, 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f670cdb",
   "metadata": {},
   "source": [
    "단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었습니다. \n",
    "이는 tokenizer.word_index를 출력하여 확인 가능합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a546f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38c3b831",
   "metadata": {},
   "source": [
    "등장 횟수가 1회인 단어들은 자연어 처리에서 배제하고자 합니다. \n",
    "이 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "801c4c70",
   "metadata": {},
   "source": [
    "단어가 약 32,000개가 존재합니다. 등장 빈도가 threshold 값인 2회 미만. 즉, 1회인 단어들은 단어 집합에서 약 42%를 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 매우 적은 수치인 약 1.2%밖에 되지 않습니다. \n",
    "아무래도 등장 빈도가 1회인 단어들은 자연어 처리에서 별로 중요하지 않을 듯 합니다. 그래서 이 단어들은 정수 인코딩 과정에서 배제시키겠습니다. 등장 빈도수가 1인 단어들의 수를 제외한 단어의 개수를 단어 집합의 최대 크기로 제한하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94bd3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63ec3f5b",
   "metadata": {},
   "source": [
    "단어 집합의 크기는 18,941개입니다. \n",
    "이를 토크나이저의 인자로 넘겨주면, 토크나이저는 텍스트 시퀀스를 숫자 시퀀스로 변환합니다. \n",
    "이러한 정수 인코딩 과정에서 이보다 큰 숫자가 부여된 단어들은 OOV로 변환하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78e63b30",
   "metadata": {},
   "source": [
    "정수 인코딩이 진행되었는지 확인하고자 X_train과 X_test에 대해서 상위 3개의 샘플만 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098f02a",
   "metadata": {},
   "source": [
    "## 8) 패딩"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bde7efe6",
   "metadata": {},
   "source": [
    "서로 다른 길이의 샘플들의 길이를 동일하게 맞춰주는 패딩 작업을 진행해보겠습니다. \n",
    "전체 데이터에서 가장 길이가 긴 리뷰와 전체 데이터의 길이 분포를 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3621de3d",
   "metadata": {},
   "source": [
    "리뷰의 최대 길이는 64, 평균 길이는 약 15입니다. \n",
    "그래프로 봤을 때, 전체적으로는 60이하의 길이를 가지는 것으로 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ac2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    count = 0\n",
    "    for sentence in nested_list:\n",
    "        if(len(sentence) <= max_len):\n",
    "            count = count + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a1d75e6",
   "metadata": {},
   "source": [
    "최대 길이가 64이므로 만약 60으로 패딩할 경우, 몇 개의 샘플들을 온전히 보전할 수 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ef2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 60\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fa523e8",
   "metadata": {},
   "source": [
    "훈련용 리뷰의 99.99%가 60이하의 길이를 가집니다. \n",
    "훈련용 리뷰를 길이 60으로 패딩하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b57cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0c392",
   "metadata": {},
   "source": [
    "# 4. BiLSTM으로 스팀 리뷰 감성 분류하기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85a7e5fa",
   "metadata": {},
   "source": [
    "하이퍼파라미터, EarlyStopping과 ModelCheckpoint와 같은 모델에 대한 상세 코드는 네이버 영화 리뷰 분류와 네이버 쇼핑 리뷰 분류 때와 크게 다르지 않습니다. \n",
    "\n",
    "하지만 이번에는 양방향 LSTM을 사용했다는 점에서 앞선 실습들과 차이를 보입니다. LSTM이 Bidirectional( ) 안에 기재되었다는 사실에 주목합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(Bidirectional(LSTM(hidden_units))) # Bidirectional LSTM을 사용\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46470d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d51bd",
   "metadata": {},
   "source": [
    "# 5. 리뷰 예측해보기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de622799",
   "metadata": {},
   "source": [
    "임의의 문장에 대한 예측을 위해서는 학습하기 전 전처리를 동일하게 적용해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = mecab.morphs(new_sentence) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e968ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('노잼 ..완전 재미 없음 ㅉㅉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626020ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('조금 어렵지만 재밌음ㅋㅋ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19cee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('케릭터가 예뻐서 좋아요')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
