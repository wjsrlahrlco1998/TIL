# Embedding



## 1. 임베딩이란?



### 1) 정의

- 자연어 처리에서 임베딩이란, 사람이 쓰는 자연어를 컴퓨터가 이해할 수 있는 숫자의 나열인 **벡터**로 바꾼 결과 혹은 그 일련의 과정을 의미한다.



## 2. 임베딩의 종류



### 1) 단어 수준의 임베딩



#### (1) 종류

- Word2Vec, GloVe, FastText, NPLM, Swivel



#### (2) 한계

-  단어 수준의 임베딩은 문장의 의미를 파악하지 못한다. 또한 동음이의어를 구분하지 못한다.



### 2) 문장 수준의 임베딩



#### (1) 종류

- ELMo(Embeddings from Language Models), BERT(Bidirectional Encoder Representations from Transformer), GPT(Generative Pre-Training)



#### (2) 장점

- 단어 시퀀스 전체의 문맥적 의미를 함축하기 때문에 단어 임베딩보다 전이 학습 효과가 좋다.



### 3) 연산 방식의 분류



#### (1) 행렬 분해 기반 방법

- 행렬 분해 기반 방법은 말뭉치 정보가 들어 있는 원래 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식의 임베딩 기법을 가리킨다.
- 분해한 이후엔 둘 중 하나의 행렬만 쓰거나 둘을 더하거나 이어 붙여 임베딩으로 사용한다.

- GloVe, Swivel 등이 여기에 속한다.



#### (2) 예측 기반 방법

- 어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어 졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습하는 방법이다.
- Word2Vec, FastText, BERT, ELMo, GPT 등이 이에 속한다.



#### (3) 토픽 기반 방법

- 주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩 수행한다.
- 각 문서가 어떤 주제 분포(topic distribution)를 갖는지 확률 벡터 형태로 변환한다.
- 잠재 디클레 할당(Latent Dirichlet Allocation) 등이 이에 속한다.

